# -*- coding: utf-8 -*-
"""site_generator_RandomForest_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oA1wfK4DFYusz9eLuYRviZ8W9OYLtDLo
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import warnings
warnings.filterwarnings('ignore')
from sklearn import metrics
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import collections
import scipy
import datetime
import traceback
import pickle
import copy
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
RS =  42
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

"""This notebook is classifies input text into 4 labels(5 labels I add label UNKNOWN). 
Model is saved by pickle to use it in Flask server. 
Auther Alex Bocharov skype bam271074
"""

from google.colab import drive
drive.mount ('/content/gdrive', force_remount = True)

data_df=pd.read_excel('../content/gdrive/My Drive/dataset_site_generator/eSiteDataset01eng-train.xlsx')
data_df.head()

val_df=pd.read_excel('../content/gdrive/My Drive/dataset_site_generator/eSiteDataset01eng-val.xlsx')
val_df.head()

test_df=pd.read_excel('../content/gdrive/My Drive/dataset_site_generator/eSiteDataset01eng-test.xlsx')
test_df.head()



!pip3 list 
 #check versions

TOKEN_RE = re.compile(r'[\w\d]+')  #regular expression to start with
#TOKEN_RE = re.compile(r'[a-z]+|-?\d*[.,]?\d+|\S')

def tokenize_text_simple_regex(txt, min_token_size=4):
    """ This func tokenize text with TOKEN_RE applied ealier """
    txt = txt.lower()
    all_tokens = TOKEN_RE.findall(txt)
    return [token for token in all_tokens if len(token) >= min_token_size]

#divide dataset to train and test
df_x, x_val, df_y, y_val=train_test_split(data_df['Description'],\
                                                data_df['Target'], test_size=0.2,random_state=RS)

pl_clf = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,
                                                      max_df=0.96,
                                                      min_df=4)),
                             ('cls', RandomForestClassifier(n_estimators=150,max_depth=250,
                                                            n_jobs=-1, \
                                                  verbose=True,random_state=RS))),
                  verbose=True)

pl_clf.fit(df_x,df_y);

print('Скор на трейне',pl_clf.score(df_x, df_y))
print('Скор по валидации',pl_clf.score(x_val, y_val))

from sklearn.metrics import accuracy_score

print('Доля верных ответов на трейне', accuracy_score(data_df['Target'], pl_clf.predict(data_df['Description'])))
print()
print('Доля верных ответов на валидации', accuracy_score(val_df['Target'], pl_clf.predict(val_df['Description'])))

#let s save our model with pickle
filename = 'model_site_generator01.pickle'
pickle.dump(pl_clf, open(filename, 'wb'))

# This is example of code how to load saved model
# load the model from disk
# loaded_model = pickle.load(open(filename, 'rb'))
# prediction = loaded_model.predict(X_test)
# print(prediction)

!ls

from google.colab import files

files.download('model_site_generator01.pickle')

#let s save tokenizer with pickle
filename = 'tokenize_text_simple_regex01.pickle'
pickle.dump(tokenize_text_simple_regex, open(filename, 'wb'))

!ls

from google.colab import files

files.download('tokenize_text_simple_regex01.pickle')

